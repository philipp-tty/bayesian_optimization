import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D  # noqa: F401  (imported for 3D plotting)import torch# --- BoTorch / GPyTorchfrom botorch.acquisition import LogExpectedImprovementfrom botorch.fit import fit_gpytorch_mllfrom botorch.models import SingleTaskGPfrom botorch.optim import optimize_acqffrom gpytorch.mlls import ExactMarginalLogLikelihoodimport logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger("bo")# --------- Torch device / dtype ---------DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")DTYPE = torch.doubletorch.set_default_dtype(DTYPE)# ==========================# Bayesian Optimization Core# ==========================class BayesianOptimization:    """    Simple BO wrapper around a SingleTaskGP + LogExpectedImprovement.    Key improvements vs. the original:      - Y is stored as shape (n,1) (what BoTorch expects everywhere).      - Re-initialize the GP each update so outcome transforms re-fit correctly.      - Acquisition optimization uses higher 'raw_samples' and 'num_restarts',        and is batched (batch_limit) to avoid CUDA CUBLAS issues.    """    def __init__(        self,        x_train: torch.Tensor,        y_train: torch.Tensor,        bounds: torch.Tensor,        maximize: bool = True,        use_outcome_transform: bool = True,    ):        # Shapes: X (n, d), Y (n, 1)        if x_train.ndim != 2:            raise ValueError("x_train must have shape (n, d).")        if y_train.ndim == 1:            y_train = y_train.unsqueeze(-1)        if y_train.ndim != 2 or y_train.shape[-1] != 1:            raise ValueError("y_train must have shape (n, 1) or (n,).")        n, d = x_train.shape        if n != y_train.shape[0]:            raise ValueError("x_train and y_train must have the same number of samples.")        # Bounds: (2, d)        bounds = bounds.to(DEVICE, DTYPE)        if bounds.ndim != 2 or bounds.shape[0] != 2 or bounds.shape[1] != d:            raise ValueError(f"`bounds` must be shape (2, d); got {tuple(bounds.shape)}.")        # Warn if few points for dimension d        if n <= (d + 1) * 2:            logger.warning(                f"Insufficient data for BO: {n} samples. "                f"For d={d}, ≥ {(d + 1) * 2} samples are recommended."            )        self.maximize = maximize        self.bounds = bounds        self.x_train = x_train.to(DEVICE, DTYPE)        self.y_train = y_train.to(DEVICE, DTYPE)  # (n, 1)        self.dim = d        self._plotting = False        self._fig = None        self._ax = None        self._line_points = None        self._line_best = None        # Build model (+ outcome transform by default) and fit        self._build_and_fit_model(use_outcome_transform=use_outcome_transform)    def _build_and_fit_model(self, use_outcome_transform: bool = True) -> None:        # Rebuild the GP so outcome transforms (e.g., Standardize) are recalibrated each iteration        if use_outcome_transform:            self.model = SingleTaskGP(self.x_train, self.y_train)        else:            self.model = SingleTaskGP(self.x_train, self.y_train, outcome_transform=None)        self.model = self.model.to(DEVICE, DTYPE)        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)        fit_gpytorch_mll(self.mll)        logger.info("GP model (re)fit complete.")    def get_next_data_points(        self,        q: int = 1,        num_restarts: int | None = None,        raw_samples: int | None = None,        batch_limit: int | None = None,        maxiter: int | None = None,    ) -> torch.Tensor:        """        Optimize LogExpectedImprovement to propose the next q points.        Defaults are tuned to be robust on multi-modal problems like Himmelblau,        while using batching to avoid GPU memory spikes.        """        # Enforce analytic LogEI validity (q must be 1)        if q != 1:            raise ValueError(                "LogExpectedImprovement supports only q=1. "                "Use a q-acquisition (e.g., qLogExpectedImprovement) if you need q>1."            )        # Defaults: robust but not crazy        nr = int(num_restarts) if num_restarts is not None else max(10, 8 * q)        rs = int(raw_samples) if raw_samples is not None else max(512, 256 * q)        bl = int(batch_limit) if batch_limit is not None else 64  # batched evals of acqf        mi = int(maxiter) if maxiter is not None else 200        # Compute best_f in the acquisition's (possibly transformed) outcome space        if hasattr(self.model, "outcome_transform") and self.model.outcome_transform is not None:            y_t, _ = self.model.outcome_transform(self.y_train)  # transform to model's space            best_f_val = y_t.max() if self.maximize else y_t.min()        else:            best_f_val = self.y_train.max() if self.maximize else self.y_train.min()        best_f = float(best_f_val.item())        # Build acquisition        acq = LogExpectedImprovement(self.model, best_f=best_f, maximize=self.maximize)        options = {            "batch_limit": bl,  # evaluate raw candidates in manageable chunks            "maxiter": mi,      # handle SciPy-style arg            "max_iter": mi,     # handle PyTorch-style arg        }        # Important: enable grad for acqf optimization        with torch.enable_grad():            self.model.eval()            self.model.likelihood.eval()            new_X, _ = optimize_acqf(                acq_function=acq,                bounds=self.bounds,                q=q,                num_restarts=nr,                raw_samples=rs,                options=options,            )        return new_X    def update_model(self, new_x: torch.Tensor, new_y: torch.Tensor, use_outcome_transform: bool = True) -> None:        if new_y.ndim == 1:            new_y = new_y.unsqueeze(-1)        new_x = new_x.to(DEVICE, DTYPE)        new_y = new_y.to(DEVICE, DTYPE)        if new_x.shape[0] != new_y.shape[0]:            raise ValueError("new_x and new_y must have the same number of samples.")        # Append and rebuild the model so transforms re-fit        self.x_train = torch.cat([self.x_train, new_x], dim=0)        self.y_train = torch.cat([self.y_train, new_y], dim=0)        self._build_and_fit_model(use_outcome_transform=use_outcome_transform)        if self._plotting:            self.update_live_plot()    # -------- Live plotting (optional) --------    def start_live_plot(self) -> None:        if self._plotting:            return        try:            plt.style.use("seaborn-v0_8-darkgrid")        except Exception:            pass        plt.ion()        self._fig, self._ax = plt.subplots(figsize=(9, 5))        self._ax.set_title("Bayesian Optimization Progress", loc="left", fontsize=12, fontweight="bold")        self._ax.set_xlabel("Iteration")        self._ax.set_ylabel("Objective value")        y_vals = self.y_train.detach().cpu().numpy().reshape(-1)        iters = np.arange(1, len(y_vals) + 1, dtype=int)        best_vals = np.maximum.accumulate(y_vals) if self.maximize else np.minimum.accumulate(y_vals)        (self._line_points,) = self._ax.plot(iters, y_vals, "o", alpha=0.85, label="Observations")        (self._line_best,) = self._ax.plot(iters, best_vals, "-", linewidth=2.0, label="Best so far")        self._ax.legend(loc="best", frameon=True)        self._ax.margins(x=0.05, y=0.15)        self._ax.grid(True, linestyle="--", alpha=0.3)        self._fig.tight_layout()        try:            self._fig.canvas.manager.set_window_title("Bayesian Optimization Progress")        except Exception:            pass        self._plotting = True        self._fig.show()        plt.pause(0.001)    def update_live_plot(self) -> None:        if not self._plotting or self._fig is None or self._ax is None:            return        y_vals = self.y_train.detach().cpu().numpy().reshape(-1)        iters = np.arange(1, len(y_vals) + 1, dtype=int)        if self.maximize:            best_vals = np.maximum.accumulate(y_vals)            best_num = float(np.max(y_vals))        else:            best_vals = np.minimum.accumulate(y_vals)            best_num = float(np.min(y_vals))        self._line_points.set_data(iters, y_vals)        self._line_best.set_data(iters, best_vals)        self._ax.relim()        self._ax.autoscale_view()        y_min, y_max = self._ax.get_ylim()        pad = 0.05 * max(1e-9, (y_max - y_min))        self._ax.set_ylim(y_min - pad, y_max + pad)        self._ax.set_title(            f"Bayesian Optimization Progress — best: {best_num:.6g}",            loc="left",            fontsize=12,            fontweight="bold",        )        self._fig.canvas.draw_idle()        try:            self._fig.canvas.flush_events()        except Exception:            pass        plt.pause(0.001)    def stop_live_plot(self, close: bool = False) -> None:        self._plotting = False        if close and self._fig is not None:            plt.close(self._fig)